{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab5\n",
    "netid: yxw190015\n",
    "\n",
    "name: Yile Wang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from numpy import zeros, dot, array\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity\n",
    "\n",
    "The first function below calculates modularity for *directed* networks and also returns the maximum modularity value $Q_{\\text{max}}$ (NetworkX's modularity function does not report the $Q_{\\text{max}}$ value). The second function calculates scalar assortativity (NetworkX's assortativity functions differ from our book definition). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(G,c):\n",
    "    d = dict()\n",
    "    for k,v in enumerate(c):\n",
    "        for n in v:\n",
    "            d[n] = k\n",
    "    L = 0\n",
    "    for u,v,data in G.edges.data():\n",
    "        L += data['weight']\n",
    "    Q, Qmax = 0,1\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if d[u] == d[v]:\n",
    "                Auv = 0\n",
    "                if G.has_edge(v,u):\n",
    "                    Auv = G[v][u]['weight']\n",
    "                Q += ( Auv - G.in_degree(u,weight='weight')*G.out_degree(v,weight='weight')/L )/L\n",
    "                Qmax -= ( G.in_degree(u,weight='weight')*G.out_degree(v,weight='weight')/L )/L\n",
    "    return Q, Qmax\n",
    "\n",
    "def scalar_assortativity(G,d):\n",
    "    x = zeros(G.number_of_nodes())\n",
    "    for i,n in enumerate(G.nodes()):\n",
    "        x[i] = d[n]\n",
    "\n",
    "    A = array(nx.adjacency_matrix(G).todense().T)\n",
    "    M = 2*A.sum().sum()\n",
    "    ki = A.sum(axis=1) #row sum is in-degree\n",
    "    ko = A.sum(axis=0) #column sum is out-degree\n",
    "    mu = ( dot(ki,x)+dot(ko,x) )/M\n",
    "\n",
    "    R, Rmax = 0, 0\n",
    "    for i in range(G.number_of_nodes()):\n",
    "        for j in range(G.number_of_nodes()):\n",
    "             R += ( A[i,j]*(x[i]-mu)*(x[j]-mu) )/M\n",
    "             Rmax += ( A[i,j]*(x[i]-mu)**2 )/M\n",
    "\n",
    "    return R, Rmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIFA exports by geographic region is assortatively mixed: 0.1200/0.5505\n",
      "FIFA exports by importers/exporters is disassortatively mixed: -0.0185/0.5748\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_weighted_edgelist('fifa1998.edgelist',create_using=nx.DiGraph)\n",
    "\n",
    "c = {\n",
    "    'group1': {'Argentina','Brazil','Chile','Mexico','Colombia','Jamaica','Paraguay'},\n",
    "    'group2': {'Japan','SouthKorea'},\n",
    "    'group3': {'UnitedStates'},\n",
    "    'group4': {'Nigeria','Morocco','SouthAfrica','Cameroon','Tunisia','Iran','Turkey'},\n",
    "    'group5': {'Scotland','Belgium','Austria','Germany','Denmark','Spain','France','GreatBritain','Greece','Netherlands','Norway','Portugal','Italy','Yugoslavia','Romania','Bulgaria','Croatia','Switzerland'}\n",
    "    }\n",
    "Q, Qmax = modularity(G,c.values())\n",
    "print('FIFA exports by geographic region is assortatively mixed: %1.4f/%1.4f' % (Q,Qmax))\n",
    "\n",
    "c = {\n",
    "    'exporters': {'Argentina','Brazil','Chile','Colombia','Mexico','Scotland','Belgium','Austria','Denmark','France','Greece','Netherlands','Portugal','Yugoslavia','Croatia','Jamaica','Cameroon','Nigeria','Morocco','Tunisia'},\n",
    "    'importers': {'Paraguay','SouthKorea','UnitedStates','SouthAfrica','Iran','Turkey','Germany','Spain','GreatBritain','Norway','Italy','Romania','Bulgaria','Switzerland','Japan'}\n",
    "    }\n",
    "Q, Qmax = modularity(G,c.values())\n",
    "print('FIFA exports by importers/exporters is disassortatively mixed: %1.4f/%1.4f' % (Q,Qmax))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing why I see the values\n",
    "Firstly I need to define what's the meaning of `Q` and `Qmax`. The `Q` means the modularity, whose definition is that the fraction of edges that connect nodes of the same type minus fraction of edges if it's random graph. If the `Q` is higher, it means that it is more likely an assortative mixing in the graph (More organized). The `Qmax` represents an ideal situation that the modularity score of every edge is between nodes of same type. \n",
    "\n",
    "In the FIFA network analysis, I can know that if we group players by geographic region,the `Q` is larger than 0, which means that there is higher fraction that similar type of nodes are connected. The real life situation is that there are more players trading within the same continent. \n",
    "\n",
    "For the second group, importers and exporters network, the modularity score `Q` is negative, which means that the edges between nodes are not likely coming from the same type of nodes, which makes sense here because the condition to make connections between nodes is that player from country A is playing at country B now. In the importers/exporters network, there may have fewer connections between nodes both in the importers/exporters type (There are more connections/edges between importers and exporters!). That's the reason why it has negative modularity scores.\n",
    "\n",
    "For the similar `Qmax`, it is because that the network is still the same network with same nodes and edges. The only thing different is the $\\delta(c_i, c_j)$, which means all the edges between nodes of same type. By grouping nodes in different way, there might be slightly different value for $\\delta(c_i, c_j)$, but overall the two `Qmax` should be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assortativity by GDP: -0.0005\n",
      "Assortativity by life expectancy: 0.1281\n",
      "Assortativity by tariff: 0.1191\n"
     ]
    }
   ],
   "source": [
    "gdp = pickle.load(open('gdp.pkl','rb'))\n",
    "life_expectancy = pickle.load(open('life_expectancy.pkl','rb'))\n",
    "tariff = pickle.load(open('tariff.pkl','rb'))\n",
    "\n",
    "G = nx.read_weighted_edgelist('world_trade_2014.edgelist',create_using=nx.DiGraph)\n",
    "\n",
    "R, Rmax = scalar_assortativity(G,gdp)\n",
    "print('Assortativity by GDP: %1.4f' % (R/Rmax))\n",
    "R, Rmax =  scalar_assortativity(G,life_expectancy)\n",
    "print('Assortativity by life expectancy: %1.4f' % (R/Rmax))\n",
    "R, Rmax =  scalar_assortativity(G,tariff)\n",
    "print('Assortativity by tariff: %1.4f' % (R/Rmax))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe world trading system and reason\n",
    "First we need to know what the edges mean in the network. From the definition of scalar assortativity, it measures how much two variables vary together. In the world trading network, the edge is defined as the trading value between two countries. \n",
    "\n",
    "The scalar assortativity is to describe the covariance between nodes $x_i$ and $x_j$. In simple words, it captures how much $x_i$ node is different from the mean. The $\\mu$ is the mean, which is the weighted average of scalar node value, proportioned to their degree. The concept of the `R/Rmax` is similar with `Q/Qmax`.\n",
    "\n",
    "From the assortativity statistics, we can know that GDP has almost no salient effect to the trading behaviors between two countries. The `R/Rmax` of the GDP is close to random graph, which means that `GDP` of a country is not an important factor to affect the trading relationship. A potential reason is that GDP doesn't mean the average wealth level for individuals. For example, Taiwan and Japanese may not have large number in `GDP` but the citizens are wealth than many of countries with high `GDP`. \n",
    "\n",
    "However, the life expectancy and tariff could be important factors, which make senses too. To some extent, the life expectancy can reflect the aging population distribution in the country. The increase of the life expectancy can also reflect the development of economic system and medical system for a country. These can be an important indicator for the value of goods they trades, then influencing the total trade values. Also, countries with similar aging population may have similar demand and requirment for most of the people, which could be an factor for the positive assortativity value here; The tariff is another important factor for trading. An equal (similiar level) of the tariff between two countries can be easier to reach an equilibrium then trading more goods to each other. If the difference of tariff between two countries are huge (I taxed you 10% and you taxed me 50%), there are less likely to have a stable and large amount of trading between two countries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic activity to show the following simplification\n",
    "\n",
    "The steps are here:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2m}\\sum_{i=1}^{n}\\sum_{j=1}^{n} A_{ij}(x_i - \\mu)(x_j - \\mu) = \\frac{1}{2m}\\sum_{i=1}^{n}\\sum_{j=1}^{n} A_{ij}(x_ix_j - \\mu x_i - \\mu x_j + \\mu ^2) \n",
    "\n",
    "$$\n",
    "The covariance equation of the $x_i$ and $x_j$ can be re-written as:\n",
    "$$\n",
    "cov(x_i, x_j) = \\mathbb{E}(x_i x_j) - \\mu{_{x_i}} \\mathbb{E}(x_j) - \\mu{_{x_j}}\\mathbb{E}(x_i) + \\mu{_{x_i}} \\mu{_{x_j}}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "cov(x_i, x_j) =\\mathbb{E}(x_i x_j) - \\mu{_{x_i}} \\mu{_{x_j}} - \\mu{_{x_j}} \\mu{_{x_i}} + \\mu{_{x_i}} \\mu{_{x_j}} \n",
    "$$\n",
    "\n",
    "We got:\n",
    "\n",
    "$$\n",
    "cov(x_i, x_j) =\\mathbb{E}(x_i x_j) - \\mu{_{x_i}} \\mu{_{x_j}}  \n",
    "$$\n",
    "\n",
    "Finally the equation is\n",
    "\n",
    "$$\n",
    "cov(x_i, x_j) =\\mathbb{E}(x_i x_j) - \\mathbb{E}(x_i) \\mathbb{E}(x_j)\n",
    "$$\n",
    "\n",
    "expected value of the $(x_i x_j)$ is \n",
    "$$\n",
    "\\mathbb{E}(x_i x_j) = \\frac{1}{2m}\\sum_{i=1}^{n}\\sum_{j=1}^{n} A_{ij}(x_i x_j) \n",
    "$$\n",
    "\n",
    "The expected value of the $x_i$ and $x_j$ is $\\mu$, then we can multiply them to $\\mu{^2}$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(x_i)(x_j) = \\frac{1}{4m^2}\\sum_{i=1}^{n} k_i x_i \\sum_{j=1}^{n} k_j x_j = \\mu{^2}  \n",
    "$$\n",
    "\n",
    "Thus, we can know that the \n",
    "\n",
    "$$\n",
    "R=\\frac{1}{2m}\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}(\\mathbb{E}[x_i x_j] - \\mathbb{E}[x_i]\\mathbb{E}[x_j]) =\\frac{1}{2m}\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}x_ix_j-\\frac{1}{4m^2}\\sum_{i=1}^{n} k_i x_i \\sum_{j=1}^{n} k_j x_j = \\frac{1}{2m}\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}x_ix_j-\\mu{^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "To read the data\n",
    "Goal:   \n",
    "1. create a raw_twitter.txt file, contains the text of a tweet on each line.\n",
    "\n",
    "2. produce hashtag_sets.txt from raw_twitter.txt. It should contain a space-delimited list of hashtags, correspond to all the hashtags appeared in one tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2 column 1 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mraw_twitter.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m         json\u001b[39m.\u001b[39;49mloads(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/tvbenv/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)"
     ]
    }
   ],
   "source": [
    "with open('raw_twitter.json', 'r') as f:\n",
    "    for line in f:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_twitter.txt', 'r', encoding='utf-8') as f:\n",
    "    with open('output', 'w', encoding='utf-8') as fout:\n",
    "        for line in f:\n",
    "            fout.write(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it mean if this file has empty lines?\n",
    "The original twitters don't have hashtags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
